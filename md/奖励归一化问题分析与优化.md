# 🔍 奖励归一化问题分析与优化方案

## 📊 关键发现

### 实际奖励尺度对比

| 指标 | num_gnbs=7 | num_gnbs=19 | 比例 | 分析 |
|------|-----------|-------------|------|------|
| **平均奖励** | 7.38 | 2.82 | **0.38x** | ❌ **过度归一化** |
| **标准差** | 0.49 | 0.38 | 0.78x | ⚠️ 波动更小 |
| **最小值** | 5.96 | 1.59 | 0.27x | ❌ 差距很大 |
| **最大值** | 8.86 | 4.04 | 0.46x | ❌ 差距很大 |

**关键结论**: ❌ **归一化过度，导致奖励信号太弱**

---

## 🔍 当前归一化分析

### 当前实现（env/myenv.py 第158-165行）

```python
# 【优化】奖励归一化：按总用户数归一化，保持奖励尺度一致
reference_num_gnbs = 7
reference_users_per_gnb = 16
reference_total_users = reference_num_gnbs * reference_users_per_gnb  # 112
current_total_users = self.num_gnbs * self.users_per_gnb
normalization_factor = current_total_users / reference_total_users
base_reward = spectrum_efficiency / (5.0 * normalization_factor)
```

### 归一化因子计算

| 配置 | 总用户数 | 归一化因子 | 分析 |
|------|---------|-----------|------|
| num_gnbs=7 | 112 | 1.0 | 基准 |
| num_gnbs=19 | 304 | **2.71** | ⚠️ **过大** |

### 问题分析

**问题1: 归一化因子过大**
- 归一化因子2.71意味着奖励被缩小了2.71倍
- 导致奖励信号太弱，Critic难以学习

**问题2: 奖励尺度不匹配**
- num_gnbs=7: 奖励范围 6-9
- num_gnbs=19: 奖励范围 2-4
- **尺度差异太大**，导致学习困难

**问题3: 信噪比过低**
- 奖励信号太弱（2-4 vs 6-9）
- Critic网络可能"躺平"，输出保守的平均值
- 导致"假收敛"（平线）

---

## ✅ 用户判断的合理性

### ✅ **有道理的部分**

1. ✅ **奖励数值膨胀确实是问题**
   - 虽然已经归一化，但归一化方式不对
   - 归一化过度导致奖励信号太弱

2. ✅ **信噪比过低导致"假收敛"**
   - 奖励信号太弱（2-4），Critic难以学习
   - 网络倾向于输出保守的平均值

3. ✅ **探索空间确实更大**
   - 19个智能体的动作组合空间确实是指数级增长
   - 需要更多探索

---

### ⚠️ **需要修正的部分**

1. ⚠️ **不是"如果没有做归一化"**
   - 归一化已经实施
   - 问题是"归一化方式不对"

2. ⚠️ **归一化过度，不是不够**
   - 当前归一化导致奖励被过度缩小
   - 需要更温和的归一化方式

---

## 💡 优化方案

### 方案1: 平方根归一化 ⭐⭐⭐⭐⭐ **最推荐**

**原理**: 使用平方根归一化，更温和地缩放奖励

**修改**（env/myenv.py 第164行）:
```python
# 原代码
normalization_factor = current_total_users / reference_total_users

# 修改为
normalization_factor = np.sqrt(current_total_users / reference_total_users)
```

**效果**:
- 归一化因子从2.71降到1.65（更温和）
- 奖励信号更强，更容易学习
- 预期奖励比例从0.38x提升到0.61x

**计算**:
- num_gnbs=7: 归一化因子 = 1.0
- num_gnbs=19: 归一化因子 = sqrt(2.71) = 1.65
- 奖励比例预期: 1.0 / 1.65 = 0.61x（比0.38x好）

---

### 方案2: 0.7次方归一化 ⭐⭐⭐⭐

**原理**: 使用0.7次方，介于线性和平方根之间

**修改**:
```python
normalization_factor = (current_total_users / reference_total_users) ** 0.7
```

**效果**:
- 归一化因子 ≈ 2.0（比2.71温和）
- 奖励比例预期: 0.5x（比0.38x好）

---

### 方案3: 固定归一化（不归一化） ⭐⭐⭐

**原理**: 如果奖励尺度已经合理，可以不归一化

**修改**:
```python
normalization_factor = 1.0  # 不归一化
base_reward = spectrum_efficiency / 5.0
```

**效果**:
- 奖励比例 = 2.71x（原始比例）
- 需要调整Critic网络来适应更大的奖励尺度

---

### 方案4: 调整归一化基准 ⭐⭐⭐

**原理**: 使用更小的归一化基准

**修改**:
```python
# 使用更小的基准（如5个智能体）
reference_num_gnbs = 5  # 从7改为5
reference_total_users = reference_num_gnbs * reference_users_per_gnb
normalization_factor = current_total_users / reference_total_users
```

**效果**:
- num_gnbs=7: 归一化因子 = 1.4
- num_gnbs=19: 归一化因子 = 3.8
- 奖励比例: 1.4 / 3.8 = 0.37x（仍然太小）

---

## 🎯 推荐实施方案

### 优先级1: 平方根归一化 ⭐⭐⭐⭐⭐ **最推荐**

**理由**:
- ✅ 更温和的归一化方式
- ✅ 奖励信号更强，更容易学习
- ✅ 预期奖励比例从0.38x提升到0.61x
- ✅ 简单易实施

**修改代码**:
```python
# env/myenv.py 第164行
normalization_factor = np.sqrt(current_total_users / reference_total_users)
```

---

### 优先级2: 增强探索策略 ⭐⭐⭐⭐

**当前配置**:
```python
self.epsilon = 0.95
self.epsilon_decay = 0.9998
self.epsilon_min = 0.05
```

**优化配置**:
```python
self.epsilon = 0.98        # 进一步增加初始探索
self.epsilon_decay = 0.9999  # 进一步减慢衰减
self.epsilon_min = 0.1     # 保持更多探索
```

---

## 📊 预期效果

### 如果实施平方根归一化

**预期改善**:
- ✅ 奖励比例: 从0.38x提升到0.61x
- ✅ 奖励信号: 从2-4提升到3-6（更强）
- ✅ 前期趋势: 从+0.118提升到>0.15（有明显上升）
- ✅ 学习效率: 从0.0141提升到>0.03（提升2倍）

**对比**:
- 当前: 平均奖励比例 0.38x（过度归一化）
- 预期: 平均奖励比例 0.61x（更合理）

---

## ✅ 结论

### 用户判断的合理性

1. ✅ **奖励数值膨胀确实是问题** - **有道理**
2. ✅ **信噪比过低导致"假收敛"** - **有道理**
3. ✅ **需要归一化** - **已经实施，但方式不对**
4. ✅ **需要降低学习率** - **已经实施**

### 需要修正的部分

1. ⚠️ **归一化已经实施，但方式不对**
2. ⚠️ **当前归一化过度，导致奖励信号太弱**
3. ⚠️ **需要调整归一化方式，而不是重新实施**

### 建议

**优先级1**: ✅ **修改env中的归一化方式**（使用平方根归一化）
**优先级2**: ✅ **增强探索策略**
**优先级3**: ✅ **使用ReduceLROnPlateau调度器**（已实施）

---

*分析时间: 2026-01-31*  
*状态: 🔍 判断有道理，需要修改env中的归一化方式*
