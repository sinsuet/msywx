# ✅ 奖励归一化优化实施说明

## 🎯 问题诊断

### 关键发现

**实际奖励尺度对比**:
- num_gnbs=7: 平均奖励 7.38
- num_gnbs=19: 平均奖励 2.82
- **比例: 0.38x** ❌ **过度归一化**

**问题**:
- ❌ 当前归一化导致奖励被过度缩小（归一化因子2.71）
- ❌ 奖励信号太弱（2-4 vs 6-9），Critic难以学习
- ❌ 导致"假收敛"（平线）

---

## ✅ 已实施的优化

### 修改: 平方根归一化

**文件**: `env/myenv.py`  
**位置**: 第164行

**修改内容**:
```python
# 原代码
normalization_factor = current_total_users / reference_total_users

# 修改为
normalization_factor = np.sqrt(current_total_users / reference_total_users)
```

**效果**:
- 归一化因子从2.71降到1.65（更温和）
- 奖励信号更强，更容易学习
- 预期奖励比例从0.38x提升到0.61x

---

## 📊 预期效果

### 如果优化成功

**预期改善**:
- ✅ 奖励比例: 从0.38x提升到0.61x
- ✅ 奖励信号: 从2-4提升到3-6（更强）
- ✅ 前期趋势: 从+0.118提升到>0.15（有明显上升）
- ✅ 学习效率: 从0.0141提升到>0.03（提升2倍）

---

## 🚀 下一步

### 重新训练测试

**运行优化后的训练**:
```bash
conda activate msyang
cd /home/hymn/yang
python main_scheduler_optimized.py
```

**预期**:
- 前期趋势应该 > 0.15（有明显上升）
- 奖励比例应该接近 0.6x（更合理）
- 学习效率应该提升

---

## ✅ 总结

### 用户判断的合理性

1. ✅ **奖励数值膨胀确实是问题** - **有道理**
2. ✅ **信噪比过低导致"假收敛"** - **有道理**
3. ✅ **需要归一化** - **已经实施，但方式不对**

### 已实施的优化

1. ✅ **修改env中的归一化方式**（使用平方根归一化）
2. ✅ **增强探索策略**（已实施）
3. ✅ **使用ReduceLROnPlateau调度器**（已实施）

---

*实施时间: 2026-01-31*  
*状态: ✅ 已修改env中的归一化方式*
